---
title: Center-Based Relaxed Learning Against Membership Inference Attacks
abstract: Membership inference attacks (MIAs) are currently considered one of the
  main privacy attack strategies, and their defense mechanisms have also been extensively
  explored. However, there is still a gap between the existing defense approaches
  and ideal models in both performance and deployment costs. In particular, we observed
  that the privacy vulnerability of the model is closely correlated with the gap between
  the model’s data-memorizing ability and generalization ability. To address it, we
  propose a new architecture-agnostic training paradigm called Center-based Relaxed
  Learning (CRL), which is adaptive to any classification model and provides privacy
  preservation by sacrificing a minimal or no loss of model generalizability. We emphasize
  that CRL can better maintain the model’s consistency between member and non-member
  data. Through extensive experiments on common classification datasets, we empirically
  show that this approach exhibits comparable performance without requiring additional
  model capacity or data costs.
openreview: unlWrunFjg
software: https://github.com/JEKimLab/UAI24_CRL
section: Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: fang24a
month: 0
tex_title: Center-Based Relaxed Learning Against Membership Inference Attacks
firstpage: 1294
lastpage: 1306
page: 1294-1306
order: 1294
cycles: false
bibtex_author: Fang, Xingli and Kim, Jung-Eun
author:
- given: Xingli
  family: Fang
- given: Jung-Eun
  family: Kim
date: 2024-09-12
address:
container-title: Proceedings of the Fortieth Conference on Uncertainty in Artificial
  Intelligence
volume: '244'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 9
  - 12
pdf: https://raw.githubusercontent.com/mlresearch/v244/main/assets/fang24a/fang24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
