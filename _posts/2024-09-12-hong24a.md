---
title: Revisiting Convergence of AdaGrad with Relaxed Assumptions
abstract: In this study, we revisit the convergence of AdaGrad with momentum (covering
  AdaGrad as a special case) on non-convex smooth optimization problems. We consider
  a general noise model where the noise magnitude is controlled by the function value
  gap together with the gradient magnitude. This model encompasses a broad range of
  noises including bounded noise, sub-Gaussian noise, affine variance noise and the
  expected smoothness, and it has been shown to be more realistic in many practical
  applications. Our analysis yields a probabilistic convergence rate which, under
  the general noise, could reach at $\tilde{\mathcal{O}}(1/\sqrt{T})$. This rate does
  not rely on prior knowledge of problem-parameters and could accelerate to $\tilde{\mathcal{O}}(1/T)$
  where $T$ denotes the total number iterations, when the noise parameters related
  to the function value gap and noise level are sufficiently small. The convergence
  rate thus matches the lower rate for stochastic first-order methods over non-convex
  smooth landscape up to logarithm terms [Arjevani et al., 2023]. We further derive
  a convergence bound for AdaGrad with momentum, considering the generalized smoothness
  where the local smoothness is controlled by a first-order function of the gradient
  norm.
openreview: AlYO5cq1fG
section: Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hong24a
month: 0
tex_title: Revisiting Convergence of AdaGrad with Relaxed Assumptions
firstpage: 1727
lastpage: 1750
page: 1727-1750
order: 1727
cycles: false
bibtex_author: Hong, Yusu and Lin, Junhong
author:
- given: Yusu
  family: Hong
- given: Junhong
  family: Lin
date: 2024-09-12
address:
container-title: Proceedings of the Fortieth Conference on Uncertainty in Artificial
  Intelligence
volume: '244'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 9
  - 12
pdf: https://raw.githubusercontent.com/mlresearch/v244/main/assets/hong24a/hong24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
