---
title: Quantization of Large Language Models with an Overdetermined Basis
abstract: In this paper, we introduce an algorithm for data quantization based on
  the principles of Kashin representation. This approach hinges on decomposing any
  given vector, matrix, or tensor into two factors. The first factor maintains a small
  infinity norm, while the second exhibits a similarly constrained norm when multiplied
  by an orthogonal matrix. Surprisingly, the entries of factors after decomposition
  are well-concentrated around several peaks, which allows us to efficiently replace
  them with corresponding centroids for quantization purposes. We study the theoretical
  properties of the proposed approach and rigorously evaluate our compression algorithm
  in the context of next-word prediction tasks, employing models like OPT of varying
  sizes. Our findings demonstrate that Kashin Quantization achieves competitive quality
  in model performance while ensuring superior data compression, marking a significant
  advancement in the field of data quantization.
openreview: 40rikZ1pCm
software: https://github.com/MerkulovDaniil/kquantize
section: Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: merkulov24a
month: 0
tex_title: Quantization of Large Language Models with an Overdetermined Basis
firstpage: 2527
lastpage: 2536
page: 2527-2536
order: 2527
cycles: false
bibtex_author: Merkulov, Daniil and Cherniuk, Daria and Rudikov, Alexander and Oseledets,
  Ivan and Muravleva, Ekaterina and Mikhalev, Aleksandr and Kashin, Boris
author:
- given: Daniil
  family: Merkulov
- given: Daria
  family: Cherniuk
- given: Alexander
  family: Rudikov
- given: Ivan
  family: Oseledets
- given: Ekaterina
  family: Muravleva
- given: Aleksandr
  family: Mikhalev
- given: Boris
  family: Kashin
date: 2024-09-12
address:
container-title: Proceedings of the Fortieth Conference on Uncertainty in Artificial
  Intelligence
volume: '244'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 9
  - 12
pdf: https://raw.githubusercontent.com/mlresearch/v244/main/assets/merkulov24a/merkulov24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
