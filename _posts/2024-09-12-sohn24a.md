---
title: Memorization Capacity for Additive Fine-Tuning with Small ReLU Networks
abstract: Fine-tuning large pre-trained models is a common practice in machine learning
  applications, yet its mathematical analysis remains largely unexplored. In this
  paper, we study fine-tuning through the lens of memorization capacity. Our new measure,
  the Fine-Tuning Capacity (FTC), is defined as the maximum number of samples a neural
  network can fine-tune, or equivalently, as the minimum number of neurons ($m$) needed
  to arbitrarily change $N$ labels among $K$ samples considered in the fine-tuning
  process. In essence, FTC extends the memorization capacity concept to the fine-tuning
  scenario. We analyze FTC for the additive fine-tuning scenario where the fine-tuned
  network is defined as the summation of the frozen pre-trained network $f$ and a
  neural network $g$ (with $m$ neurons) designed for fine-tuning. When $g$ is a ReLU
  network with either 2 or 3 layers, we obtain tight upper and lower bounds on FTC;
  we show that $N$ samples can be fine-tuned with $m=\Theta(N)$ neurons for 2-layer
  networks, and with $m=\Theta(\sqrt{N})$ neurons for 3-layer networks, no matter
  how large $K$ is. Our results recover the known memorization capacity results when
  $N = K$ as a special case.
openreview: x1Y3x79J42
section: Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sohn24a
month: 0
tex_title: Memorization Capacity for Additive Fine-Tuning with Small ReLU Networks
firstpage: 3264
lastpage: 3278
page: 3264-3278
order: 3264
cycles: false
bibtex_author: Sohn, Jy-yong and Kwon, Dohyun and An, Seoyeon and Lee, Kangwook
author:
- given: Jy-yong
  family: Sohn
- given: Dohyun
  family: Kwon
- given: Seoyeon
  family: An
- given: Kangwook
  family: Lee
date: 2024-09-12
address:
container-title: Proceedings of the Fortieth Conference on Uncertainty in Artificial
  Intelligence
volume: '244'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 9
  - 12
pdf: https://raw.githubusercontent.com/mlresearch/v244/main/assets/sohn24a/sohn24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
