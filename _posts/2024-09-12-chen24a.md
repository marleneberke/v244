---
title: 'Gradient descent in matrix factorization: Understanding large initialization'
abstract: Gradient Descent (GD) has been proven effective in solving various matrix
  factorization problems. However, its optimization behavior with large initial values
  remains less understood. To address this gap, this paper presents a novel theoretical
  framework for examining the convergence trajectory of GD with a large initialization.
  The framework is grounded in signal-to-noise ratio concepts and inductive arguments.
  The results uncover an implicit incremental learning phenomenon in GD and offer
  a deeper understanding of its performance in large initialization scenarios.
openreview: zHplexWZSr
section: Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen24a
month: 0
tex_title: 'Gradient descent in matrix factorization: Understanding large initialization'
firstpage: 619
lastpage: 647
page: 619-647
order: 619
cycles: false
bibtex_author: Chen, Hengchao and Chen, Xin and Elmasri, Mohamad and Sun, Qiang
author:
- given: Hengchao
  family: Chen
- given: Xin
  family: Chen
- given: Mohamad
  family: Elmasri
- given: Qiang
  family: Sun
date: 2024-09-12
address:
container-title: Proceedings of the Fortieth Conference on Uncertainty in Artificial
  Intelligence
volume: '244'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 9
  - 12
pdf: https://raw.githubusercontent.com/mlresearch/v244/main/assets/chen24a/chen24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
