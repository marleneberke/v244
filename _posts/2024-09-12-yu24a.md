---
title: Offline Reward Perturbation Boosts Distributional Shift in Online RL
abstract: Offline-to-online reinforcement learning has recently been shown effective
  in reducing the online sample complexity by first training from offline collected
  data. However, this additional data source may also invite new poisoning attacks
  that target offline training. In this work, we reveal such vulnerabilities in <em>critic-regularized</em>
  offline RL by proposing a novel data poisoning attack method, which is stealthy
  in the sense that the performance during the offline training remains intact, but
  the online fine-tuning stage will suffer a significant performance drop. Our method
  leverages the techniques from bi-level optimization to promote the over-estimation/distribution
  shift under offline-to-online reinforcement learning. Experiments on four environments
  confirm the satisfaction of the new stealthiness requirement, and can be effective
  in attacking with only a small budget and without having white-box access to the
  victim model.
openreview: wbwTF909Ve
section: Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yu24a
month: 0
tex_title: Offline Reward Perturbation Boosts Distributional Shift in Online RL
firstpage: 4041
lastpage: 4055
page: 4041-4055
order: 4041
cycles: false
bibtex_author: Yu, Zishun and Kang, Siteng and Zhang, Xinhua
author:
- given: Zishun
  family: Yu
- given: Siteng
  family: Kang
- given: Xinhua
  family: Zhang
date: 2024-09-12
address:
container-title: Proceedings of the Fortieth Conference on Uncertainty in Artificial
  Intelligence
volume: '244'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 9
  - 12
pdf: https://raw.githubusercontent.com/mlresearch/v244/main/assets/yu24a/yu24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
