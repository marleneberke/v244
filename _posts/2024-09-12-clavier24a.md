---
title: Towards Minimax Optimality of Model-based Robust Reinforcement Learning
abstract: We study the sample complexity of obtaining an $\epsilon$-optimal policy
  in <em>Robust</em> discounted Markov Decision Processes (RMDPs), given only access
  to a generative model of the nominal kernel. This problem is widely studied in the
  non-robust case, and it is known that any planning approach applied to an empirical
  MDP estimated with $\tilde{\mathcal{O}}(\frac{H^3  |S||A|}{\epsilon^2})$ samples
  provides an $\epsilon$-optimal policy, which is minimax optimal. Results in the
  robust case are much more scarce. For $sa$- (resp $s$-) rectangular uncertainty
  sets, until recently the best-known sample complexity was $\tilde{\mathcal{O}}(\frac{H^4  |S|^2|A|}{\epsilon^2})$
  (resp. $\tilde{\mathcal{O}}(\frac{H^4  | S |^2| A |^2}{\epsilon^2})$), for specific
  algorithms and when the uncertainty set is based on the total variation (TV), the
  KL or the Chi-square divergences. In this paper, we consider uncertainty sets defined
  with an $L_p$-ball (recovering the TV case), and study the sample complexity of
  <em>any</em> planning algorithm (with high accuracy guarantee on the solution) applied
  to an empirical RMDP estimated using the generative model. In the general case,
  we prove a sample complexity of $\tilde{\mathcal{O}}(\frac{H^4  | S || A |}{\epsilon^2})$
  for both the $sa$- and $s$-rectangular cases (improvements of $| S |$ and $| S ||
  A |$ respectively). When the size of the uncertainty is small enough, we improve
  the sample complexity to $\tilde{\mathcal{O}}(\frac{H^3 | S || A | }{\epsilon^2})$,
  recovering the lower-bound for the non-robust case for the first time and a robust
  lower-bound. Finally, we also introduce simple and efficient algorithms for solving
  the studied $L_p$ robust MDPs.
openreview: mcmMbWLkfQ
section: Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: clavier24a
month: 0
tex_title: Towards Minimax Optimality of Model-based Robust Reinforcement Learning
firstpage: 820
lastpage: 855
page: 820-855
order: 820
cycles: false
bibtex_author: Clavier, Pierre and Le Pennec, Erwan and Geist, Matthieu
author:
- given: Pierre
  family: Clavier
- given: Erwan
  family: Le Pennec
- given: Matthieu
  family: Geist
date: 2024-09-12
address:
container-title: Proceedings of the Fortieth Conference on Uncertainty in Artificial
  Intelligence
volume: '244'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 9
  - 12
pdf: https://raw.githubusercontent.com/mlresearch/v244/main/assets/clavier24a/clavier24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
