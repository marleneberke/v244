---
title: Online Policy Optimization for Robust Markov Decision Process
abstract: Reinforcement learning (RL) has exceeded human performance in many synthetic
  settings such as video games and Go. However, real-world deployment of end-to-end
  RL models is less common, as RL models can be very sensitive to perturbations in
  the environment. The robust Markov decision process (MDP) framework—in which the
  transition probabilities belong to an uncertainty set around a nominal model—provides
  one way to develop robust models. While previous analysis for robust MDP shows RL
  algorithms are effective assuming access to a generative model, it remains unclear
  whether RL can be efficient under a more realistic online setting, which requires
  a careful balance between exploration and exploitation. In this work, we consider
  online robust MDP by interacting with an unknown nominal system. We propose a robust
  optimistic policy optimization algorithm that is provably efficient. To address
  the additional uncertainty caused by an adversarial environment, our model features
  a new optimistic update rule derived via Fenchel conjugates. Our analysis establishes
  the first regret bound for online robust MDPs.
openreview: VS8EPaCSY1
section: Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dong24a
month: 0
tex_title: Online Policy Optimization for Robust Markov Decision Process
firstpage: 1146
lastpage: 1175
page: 1146-1175
order: 1146
cycles: false
bibtex_author: Dong, Jing and Li, Jingwei and Wang, Baoxiang and Zhang, Jingzhao
author:
- given: Jing
  family: Dong
- given: Jingwei
  family: Li
- given: Baoxiang
  family: Wang
- given: Jingzhao
  family: Zhang
date: 2024-09-12
address:
container-title: Proceedings of the Fortieth Conference on Uncertainty in Artificial
  Intelligence
volume: '244'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 9
  - 12
pdf: https://raw.githubusercontent.com/mlresearch/v244/main/assets/dong24a/dong24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
