---
title: Adaptive Time-Stepping Schedules for Diffusion Models
abstract: 'This paper studies how to tune the stepping schedule in diffusion models,
  which is mostly fixed in current practice, lacking theoretical foundations and assurance
  of optimal performance at the chosen discretization points. In this paper, we advocate
  the use of adaptive time-stepping schedules and design two algorithms with an optimized
  sampling error bound $EB$: (1) for continuous diffusion, we treat $EB$ as the loss
  function to discretization points and run gradient descent to adjust them; and (2)
  for discrete diffusion, we propose a greedy algorithm that adjusts only one discretization
  point to its best position in each iteration. We conducted extensive experiments
  that show (1) improved generation ability in well-trained models, and (2) premature
  though usable generation ability in under-trained models. The code is submitted
  and will be released publicly.'
openreview: lZzriJH2DC
section: Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen24c
month: 0
tex_title: Adaptive Time-Stepping Schedules for Diffusion Models
firstpage: 685
lastpage: 697
page: 685-697
order: 685
cycles: false
bibtex_author: Chen, Yuzhu and He, Fengxiang and Fu, Shi and Tian, Xinmei and Tao,
  Dacheng
author:
- given: Yuzhu
  family: Chen
- given: Fengxiang
  family: He
- given: Shi
  family: Fu
- given: Xinmei
  family: Tian
- given: Dacheng
  family: Tao
date: 2024-09-12
address:
container-title: Proceedings of the Fortieth Conference on Uncertainty in Artificial
  Intelligence
volume: '244'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 9
  - 12
pdf: https://raw.githubusercontent.com/mlresearch/v244/main/assets/chen24c/chen24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
