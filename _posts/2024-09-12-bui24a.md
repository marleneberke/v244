---
title: Revisiting Kernel Attention with Correlated Gaussian Process Representation
abstract: Transformers have increasingly become the de facto method to model sequential
  data with state-of-the-art performance. Due to its widespread use, being able to
  estimate and calibrate its modeling uncertainty is important to understand and design
  robust transformer models. To achieve this, previous works have used Gaussian processes
  (GPs) to perform uncertainty calibration for the attention units of transformers
  and attained notable successes. However, such approaches have to confine the transformers
  to the space of symmetric attention to ensure the necessary symmetric requirement
  of their GPâ€™s kernel specification, which reduces the representation capacity of
  the model. To mitigate this restriction, we propose the Correlated Gaussian Process
  Transformer (CGPT), a new class of transformers whose self-attention units are modeled
  as cross-covariance between two correlated GPs (CGPs). This allows asymmetries in
  attention and can enhance the representation capacity of GP-based transformers.
  We also derive a sparse approximation for CGP to make it scale better. Our empirical
  studies show that both CGP-based and sparse CGP-based transformers achieve better
  performance than state-of-the-art GP-based transformers on a variety of benchmark
  tasks.
openreview: xlIK0vu3MW
section: Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bui24a
month: 0
tex_title: Revisiting Kernel Attention with Correlated Gaussian Process Representation
firstpage: 450
lastpage: 470
page: 450-470
order: 450
cycles: false
bibtex_author: Bui, Long Minh and Tran Huu, Tho and Dinh, Duy and Nguyen, Tan Minh
  and Hoang, Trong Nghia
author:
- given: Long Minh
  family: Bui
- given: Tho
  family: Tran Huu
- given: Duy
  family: Dinh
- given: Tan Minh
  family: Nguyen
- given: Trong Nghia
  family: Hoang
date: 2024-09-12
address:
container-title: Proceedings of the Fortieth Conference on Uncertainty in Artificial
  Intelligence
volume: '244'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 9
  - 12
pdf: https://raw.githubusercontent.com/mlresearch/v244/main/assets/bui24a/bui24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
